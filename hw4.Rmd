---
title: "BIOST 527 HW4"
author: "Yichen"
date: "5/29/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = TRUE, warning = FALSE)
library(Matrix)
library(KernSmooth)
library(splines)
library(ggplot2)
library(dplyr)
library(glmnet)
library(regpro)
```

# Question 1(b)

I used the following three estimators: multivariate linear model, additive model using basic expansion: polynomial regression on each dimension of x (I considered polynomial degrees of 2 to 5 and found the oracle one to use), and  multivariate Nadaraya-Watson with gaussian kernel (I considered bandwidths between .1 and 0.5 and found the oracle one.)

In the first three figures below, I ploted the estimated y against the first dimension of x, $x_1$. The true regression function is plotted in black. For a clearer view of the estimators, in the last three figures, I repeated the simulation 10 times and summarized the MSE corresponding to each estimator.

In setting a, the multivariate linear model had the minimum mse. This was expected since the true underlying function was a linear addition of each of the dimension.
In setting b, the polynomial additive model performed the best because the three elements used to construct y could all be approximated by a polynomial of degree 3 or higher. 
In setting c, the multivariate Nadaraya-Watson outperformed the multivariate linear and polynomial regression probably due to its flexibility.

However, as we increase dimensions, we might start to see the nw (or some other non-parametric estimators such as k-nearest neighbors without the additive space assumption) to perform worse because we will find fewer data points in the "neighbor". Given the same sample size n and plot them into spaces with different dimensions, the space with lower dimension have higher density (points are closer to each other) but high-dimension places will have much lower density and the data becomes much sparse. This could result in a much slower convergence rate.

```{r, echo = FALSE}
rm(list = ls())

set.seed(47)

##-------- estimated y --------##
one.simu <- function(ff, n){
  
  x <- matrix(runif(n * 3,0,1), ncol = 3)
  x <- x[order(x[,1]), ]
  if (ff == "a") f = x[,1]+ x[,2] +x[,3]
  if (ff == "b") f = sin(4*x[,1]) + 2 * sqrt(x[,2]) + exp(x[,3])
  if (ff == "c") f = (x[,1] * x[,2] * x[,3])^(1/3)
  
  y <- f + rnorm(n)
  
  #The multivariate linear model estimate
  fit.y_lm <- lm(y~x)$fitted.values
  
  # An additive model estimate of your choice (eg. basis expansion with a growing number of elements)
  seq.p = seq(2,5, length = 4) # sequence of bandwidths for NW
  mse_pl <- sapply(seq.p, function(p.val) {
    fit.y_pll <- lm(y~poly(x[,1],p.val) + poly(x[,2],p.val) + poly(x[,3],p.val))$fitted.values
    mean((fit.y_pll - f)^2)})
  fit.y_pl <- lm(y~poly(x[,1],seq.p[which.min(mse_pl)]) + poly(x[,2],seq.p[which.min(mse_pl)]) +
                   poly(x[,3],seq.p[which.min(mse_pl)]))$fitted.values
  
  # A general nonparametric estimator of your choice (eg. Nadaraya-Watson or local polynomial estimation)
  x_t <- t(x)
  x_l <- split(x_t, rep(1:ncol(x_t), each = nrow(x_t)))
  seq.h = seq(0.1, 0.5, length = 100) # sequence of bandwidths for NW
  mse_NW <- sapply(seq.h, function(h.val) {
    fit.y_nww <- sapply(x_l, function(i) kernesti.regr(i,x,y,h=h.val, kernel="gauss", g=NULL, gernel="gauss", vect=FALSE))
    mean((fit.y_nww - f)^2)})
  fit.y_nw <- sapply(x_l, function(i) kernesti.regr(i,x,y,h=seq.h[which.min(mse_NW)], kernel="gauss", g=NULL, gernel="gauss", vect=FALSE))
  list("x" = x[,1], "f" = f, "lm" = fit.y_lm, "pl" = fit.y_pl, "nw" = fit.y_nw)
}

# plot
r <- one.simu("a", n = 100)
plot(r[['x']], r[['f']], type = 'l', xlab = "X1", ylab = "Estimated y", main = "Estimated y for function a")
lines(r[['x']], r[['lm']], type = 'l', col = 'red')
lines(r[['x']], r[['pl']], type = 'l', col = 'blue')
lines(r[['x']], r[['nw']], type = 'l', col = 'orange')
legend(0, 2.7, legend=c("unknown f", "multivariate linear", "poly expansion", "nw-gaussian"),
       col=c("black", "red", "blue", "orange"), lty=1:2, cex=0.8)

r <- one.simu("b", n = 100)
plot(r[['x']], r[['f']], type = 'l', xlab = "X1", ylab = "Estimated y", main = "Estimated y for function b")
lines(r[['x']], r[['lm']], type = 'l', col = 'red')
lines(r[['x']], r[['pl']], type = 'l', col = 'blue')
lines(r[['x']], r[['nw']], type = 'l', col = 'orange')
legend(0, 2.7, legend=c("unknown f", "multivariate linear", "poly expansion", "nw-gaussian"),
       col=c("black", "red", "blue", "orange"), lty=1:2, cex=0.8)

r <- one.simu("c", n = 100)
plot(r[['x']], r[['f']], type = 'l', xlab = "X1", ylab = "Estimated y", main = "Estimated y for function c")
lines(r[['x']], r[['lm']], type = 'l', col = 'red')
lines(r[['x']], r[['pl']], type = 'l', col = 'blue')
lines(r[['x']], r[['nw']], type = 'l', col = 'orange')
legend(0, 2.7, legend=c("unknown f", "multivariate linear", "poly expansion", "nw-gaussian"),
       col=c("black", "red", "blue", "orange"), lty=1:2, cex=0.8)


##-------- MSE for different estimators --------##
one.simu.mse <- function(ff, n){
  res <- one.simu(ff, n)
  mse.lm <- mean((res[['lm']] - res[['f']])^2)
  mse.pl <- mean((res[['pl']] - res[['f']])^2)
  mse.nw <- mean((res[['nw']] - res[['f']])^2)
  c(mse.lm, mse.pl, mse.nw)
}

mma <- replicate(10, one.simu.mse("a", n = 100))
mmb <- replicate(10, one.simu.mse("b", n = 100))
mmc <- replicate(10, one.simu.mse("c", n = 100))
mma2 <- t(mma)
mmb2 <- t(mmb)
mmc2 <- t(mmc)
colnames(mma2) <- colnames(mmb2) <- colnames(mmc2) <- c("multivariate linear", "basic expansion", "nw-gaussian")

# plot
boxplot(mma2, xlab = "Estimator", ylab = "MSE", main = "MSE using three estimators for function a")
boxplot(mmb2, xlab = "Estimator", ylab = "MSE", main = "MSE using three estimators for function a")
boxplot(mmc2, xlab = "Estimator", ylab = "MSE", main = "MSE using three estimators for function a")
```
